{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Step 1 - Read and Import the Archived Twitter Data\n",
    "\n",
    "The first step is to read and import all the archived data.  Download \"spritzer\" archived data from \n",
    "Raw data from:  https://archive.org/details/twitterstream\n",
    "The data is quite large, and is stored in directory/file format as:\n",
    "./yyyy/mm/dd/hh/{00-99}.json.bz2 \n",
    "\n",
    "Since our dataframes will overload the computer memory if we read it all in at once, we'll need\n",
    "to be careful about memory management.  For example, we can read in one directory at a time,\n",
    "discard data that we don't want or need in foreseeable future, and save to a csv file; then\n",
    "dump or re-use memory and go again.\n",
    "\n",
    "After downloading hte data you want to analyze, run the portions of this file first, then garbage collect\n",
    "or refresh your kernel to free up memory.  A csv from this ipynb file will be used as the basis for \n",
    "further analysis in parts 2 and 3."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necssary modules\n",
    "import pandas as pd \n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import bz2\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a 'place' or a 'coordinates' are included in a tweet\n",
    "# One or both can exist in a tweet.  For this code, 'place' is checked first and, \n",
    "# if it exists, returns true before 'coordinates' is checked\n",
    "\n",
    "def does_this_tweet_have_a_place(tweet):\n",
    "    \"\"\"Function to check if a 'place' or a 'coordinates' are included in a tweet\"\"\"\n",
    "\n",
    "    if tweet['place']:\n",
    "        country_code = (tweet['place']['country_code'])\n",
    "        #print(\"country code: \" + country_code)\n",
    "        return True\n",
    "    elif tweet['coordinates']:\n",
    "        #print(\"geo coordinates: {}\".format(tweet['coordinates']))\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in all the tweets from any one bz-zipped json  file\n",
    "def read_tweets_from_bzfile(filename):\n",
    "    \"\"\"Function read in all the tweets from any one bz-zipped json  file\"\"\"\n",
    "\n",
    "    # local variables\n",
    "    tweets = []\n",
    "    read_count = 0\n",
    "    kept_count = 0\n",
    "\n",
    "    # open and unzip the bz2 file\n",
    "    with bz2.open(filename, \"rb\") as data_file:\n",
    "        for line in data_file:\n",
    "            try: \n",
    "                # load the tweet on this line of the file\n",
    "                tweet = json.loads(line)\n",
    "                read_count += 1\n",
    "                #print(tweet['text'])\n",
    "\n",
    "                # check if the tweet has a place or geo coordinates\n",
    "                if does_this_tweet_have_a_place(tweet) :\n",
    "                    tweet['file_path'] = filename\n",
    "                    tweets.append(tweet)\n",
    "                    kept_count += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # print some outputs so we can watch it working\n",
    "    print(\"file read: {}\".format(filename))\n",
    "    print(\" tweets read in file: {}\".format(read_count))\n",
    "    print(\" tweets kept from file: {} \".format(kept_count))\n",
    "    if read_count != 0:\n",
    "        print(\" kept tweets rate: {:0>2f} %\".format(100*kept_count/read_count))\n",
    "\n",
    "    return tweets, read_count, kept_count\n",
    "\n",
    "### uncomment and run this to test/debug the read_tweets_from_bzfile function using a single file\n",
    "#tweets = []\n",
    "#read_tweets_from_bzfile(\"00.json.bz2\", tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate through a directory, get all the archive files, and then \n",
    "# read them in one at a time\n",
    "def read_tweets_from_datetimehour_dir(rootdir):\n",
    "    \"\"\"Function to iterate through a directory, get all the archive files, and then read them in one at a time\"\"\"\n",
    "\n",
    "    # declare variables\n",
    "    tweets = [] # keep tweets as an array for now for mem management\n",
    "    num_read = 0\n",
    "    num_kept = 0\n",
    "\n",
    "    # will count the number of files as we go\n",
    "    num_files_read = 0\n",
    "\n",
    "    # iterate through the directories\n",
    "    for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "        #print(directory)\n",
    "\n",
    "        # iterate through the filenames\n",
    "        for filename in filenames:\n",
    "            full_path_filename = os.path.join(directory, filename)\n",
    "\n",
    "            # call the read tweets function and keep track of counters\n",
    "            tw, nr, nk = read_tweets_from_bzfile(full_path_filename)\n",
    "\n",
    "            # append to the tweets array\n",
    "            tweets.extend( tw )  # important to use \"extend\" method \n",
    "\n",
    "            # increment the counters\n",
    "            num_files_read += 1  \n",
    "            num_read += nr\n",
    "            num_kept += nk\n",
    "\n",
    "            print(\" files read so far in this dir: {}\".format(num_files_read))\n",
    "            print(\" results so far in this dir: {} tweets\".format(len(tweets)))\n",
    "\n",
    "    print(\"done. size of tweets array: {}\".format(len(tweets)))\n",
    "\n",
    "    return tweets, num_read, num_kept, num_files_read  # return stats with the tweets array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file exists; was used in development\n",
    "def check_if_output_file_exists(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"file {} exists.\".format(filepath))\n",
    "        while True:\n",
    "            if os.path.isfile( filepath ):\n",
    "                overwrite = input('Delete old file? (If no, output will be appended)\\n Y = yes, N = no\\n')\n",
    "                if 'y' in overwrite.lower():\n",
    "                    os.remove(filepath)\n",
    "                    return False\n",
    "                elif 'n' in overwrite.lower():\n",
    "                    return True\n",
    "                else:\n",
    "                    print('input not understood; please try again')\n",
    "                    "
   ]
  },
  {
   "source": [
    "# Main run block\n",
    "Now that we've defined some key functions, we can run through it all.  This will take a while.\n",
    "\n",
    "The current set up is to run a week's worth of data.  The data should be defined in the initial declarations of \n",
    "this block, with year, month, day, and hour.  Change according to the data you downloaded and want to analyze.\n",
    "It's a good idea to restrict this to a smaller range for testing and verification before embarking on the\n",
    "entire run you want to do."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# MAIN BLOCK\n",
    "\n",
    "# instead of using os.walk, we'll specifically declare what we want to iterate through\n",
    "# so that we have control of the size of this job, and to be flexible when we want to do\n",
    "# smaller test runs\n",
    "\n",
    "# we'll also create the df and save off the results by the hour, which is about the right size to not crash\n",
    "# everything on a pentium i5 with 8GB of RAM\n",
    "# but for sanity we'll make csv files by the day, so 7 files for the week\n",
    "\n",
    "# set these variables to determine which directories will be read\n",
    "# in this example, we are going with 1 week in December 2020\n",
    "year = 2020\n",
    "month = 12\n",
    "day_start = 1\n",
    "day_end = 7\n",
    "hour_start = 0  # possible range: 0-23\n",
    "hour_end = 23\n",
    "\n",
    "# counters\n",
    "total_tweets_read = 0\n",
    "total_tweets_kept = 0\n",
    "total_files_read = 0 \n",
    "\n",
    "# other variables\n",
    "dir = \"\"\n",
    "output_csv_file = \"tweets_with_places\"\n",
    "\n",
    "tic = time.perf_counter() # start a timer\n",
    "\n",
    "# now start iterating through files\n",
    "for day in range(day_start, day_end +1):\n",
    "    \n",
    "    # the dir/file structure is hard coded\n",
    "    output_csv_file = \"tweets_with_places_\" + \\\n",
    "        str('{:0>4d}').format(year) + \\\n",
    "        str('{:0>2d}').format(month) + \\\n",
    "        str('{:0>2d}').format(day) + \\\n",
    "        \".csv\"\n",
    "    \n",
    "    write_csv_header = True  # start with true, change to false after first write-out\n",
    "    \n",
    "    for hour in range (hour_start, hour_end +1):\n",
    "\n",
    "        dir = os.path.join(str('{:0>4d}').format(year), \\\n",
    "              str('{:0>2d}').format(month), \\\n",
    "              str('{:0>2d}').format(day), \\\n",
    "              str('{:0>2d}').format(hour))\n",
    "        print(\"starting new directory: \" + dir)\n",
    "\n",
    "        if os.path.exists(dir) == False:\n",
    "            print(\"directory does not exist; moving on\")\n",
    "            break\n",
    "\n",
    "        # read the file and get back only those witih places or geo coordinates\n",
    "        tweets, tweets_read, tweets_kept, files_read = read_tweets_from_datetimehour_dir(dir)\n",
    "        tweets_df = pd.DataFrame( tweets )\n",
    "         \n",
    "        # print some outputs and statistics\n",
    "        #print(tweets_df.columns)\n",
    "        print(\"total tweets: {}\".format(tweets_read))\n",
    "        tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'])\n",
    "        print(\"date time range: {} to {}\".format(\\\n",
    "            tweets_df['created_at'].min(),tweets_df['created_at'].max()))\n",
    "        try:\n",
    "            print(\"  percentage tweets kept for {} d {} h: {:0>2f} %\".format(day, hour, 100*tweets_kept/tweets_read ))\n",
    "        except:\n",
    "            print(\"  no tweets read \")\n",
    "\n",
    "        # increment the counters\n",
    "        total_tweets_read = total_tweets_read + tweets_read\n",
    "        total_tweets_kept = total_tweets_kept + tweets_kept\n",
    "        total_files_read = total_files_read + files_read\n",
    "\n",
    "        # we can still keep lots of information from the tweet while dropping lots of extraneous or \n",
    "        # repeated information; this saves file size\n",
    "        if len(tweets_df) > 0:\n",
    "            filtered_df = tweets_df[[\\\n",
    "                'created_at','id','text','source','user',\\\n",
    "                'geo','coordinates','place','entities','lang','file_path']]\n",
    "\n",
    "            # write to the csv file\n",
    "            filtered_df.to_csv(output_csv_file, mode='a', header=write_csv_header)\n",
    "\n",
    "            write_csv_header = False  # don't write headers after the first time\n",
    "\n",
    "            print(\"wrote to file\")\n",
    "        else:\n",
    "            print(\"nothing written to file\")\n",
    "\n",
    "        # print stats for the 'hour' read in\n",
    "        print(\"hour {} ended\".format(hour))\n",
    "        print(\"TOTAL tweets kept, tweets read: {}, {}\".format(total_tweets_kept, total_tweets_read))\n",
    "        print(\"TOTAL files read: {}\".format(total_files_read))\n",
    "        print(\"TOTAL percentage tweets kept: {:0>2f} %\".format( 100*total_tweets_kept/total_tweets_read ))\n",
    "\n",
    "    # print stats for the 'day' read in\n",
    "    print(\"day {} ended\".format(day))\n",
    "    print(\"TOTAL tweets kept, tweets read: {}, {}\".format(total_tweets_kept, total_tweets_read))\n",
    "    print(\"TOTAL files read: {}\".format(total_files_read))\n",
    "    print(\"TOTAL percentage tweets kept: {:0>2f} %\".format( 100*total_tweets_kept/total_tweets_read ))\n",
    "\n",
    "# print overall stats\n",
    "print(\"all files read\")\n",
    "print(\"TOTAL tweets kept, tweets read: {}, {}\".format(total_tweets_kept, total_tweets_read))\n",
    "print(\"TOTAL files read: {}\".format(total_files_read))\n",
    "print(\"TOTAL percentage tweets kept: {:0>2f} %\".format( 100*total_tweets_kept/total_tweets_read ))\n",
    "        \n",
    "# how long did that take?\n",
    "toc = time.perf_counter()\n",
    "print(f\"iterating and determining place from geo coords took {toc - tic:0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}