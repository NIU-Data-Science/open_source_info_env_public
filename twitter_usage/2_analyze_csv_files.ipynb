{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0e04390b745d7540077d5afc7ea9b350ff3db0922faee759c3145cf85f5ee4c0c",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Step 2 - analyze the geo-tagged and place-tagged tweets\n",
    "\n",
    "We are assuming tha all the archived tweets were read in in step 1, and an output csv file \n",
    "was writtten that contains only those tweets with a 'place' or a 'geo cordinates' included.\n",
    "\n",
    "Now, we will start with that csv file, and work up some statistics and basic bar graphs\n",
    "so we can get a better idea of the data by country or region or time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import pandas as pd \n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import bz2\n",
    "import gc\n",
    "import pytz\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install reverse_geocoder\n",
    "import reverse_geocoder\n",
    "\n",
    "# !pip install pycountry-convert\n",
    "import pycountry_convert as pcountry\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Geopandas\n",
    "\n",
    "# There were lots of problems with getting geopandas to run consistently; it might fail one day or another. \n",
    "It's recommended to create an environment, install geopandas and try it.  If it doesn't work, try updating it,\n",
    "uninstalling and reinstalling it, and anything else suggested on Stack overflow.  I found conda-forge to have the \n",
    "best successrate, but it would still break some days and require re-installation.\n",
    "\n",
    "# !conda install -c conda-forge geopandas  (geopandas is a pain to install, use conda-forge for best chances of success)\n",
    "# !pip install descartes\n",
    "import descartes\n",
    "import geopandas\n",
    "\n",
    "# if you are having trouble, you can try upgrading and running with this command\n",
    "# !pip install geopandas --upgrade \n",
    "\n",
    "# or, you can uninstall geopandas and start over again, probably using conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all csv files in this working directory\n",
    "# they should be in there as one csv file per day but this will rather dumbly\n",
    "# just import everything it sees in the current working directory\n",
    "\n",
    "_, _, all_files_in_dir = next(os.walk(\".\"))\n",
    "csv_files = [ fi for fi in all_files_in_dir if (fi.lower().endswith(\".csv\") and (\"copy\" not in fi.lower())) ]\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now read in all those csv files and create one big pandas dataframe\n",
    "tweets = pd.DataFrame()\n",
    "\n",
    "tic = time.perf_counter() # start a timer\n",
    "\n",
    "#tweets = [pd.read_csv(f) for f in csv_files]\n",
    "for f in csv_files:\n",
    "    tw = pd.read_csv(f, dtype={'id': str}, parse_dates=['created_at']) \n",
    "    print(\"file {}, tweets {}\". format(f, len(tw)))\n",
    "    tweets = tweets.append( tw )\n",
    "    \n",
    "print(\"total tweets read in: {}\".format(len(tweets)))\n",
    "      \n",
    "tweets.drop_duplicates()\n",
    "\n",
    "gc.collect(2)\n",
    "\n",
    "#print(tweets.describe())\n",
    "#print(tweets.head())\n",
    "#print(tweets.tail())\n",
    "#print(tweets.columns)\n",
    "print(tweets.info())\n",
    "\n",
    "# how long did that take?\n",
    "toc = time.perf_counter()\n",
    "print(f\"reading all the files took {toc - tic:0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The tweets df has several lines with no places, but geo coordinates instead\n",
    "# so let's look up the place name and country code given the geo coordinates\n",
    "# and set that value back in the original dataframe\n",
    "\n",
    "tic = time.perf_counter() # start a timer\n",
    "\n",
    "coords_list = pd.DataFrame( tweets[tweets['place'].isna()][['id','coordinates']] )\n",
    "\n",
    "for _, row in coords_list.iterrows():\n",
    "\n",
    "    # find the lat and lon within the row\n",
    "    line = eval(row['coordinates'])                 # read the row string in as a dict \n",
    "    lat = line['coordinates'][1]                    # twitter stores lat and lon backwards\n",
    "    lon = line['coordinates'][0]\n",
    "\n",
    "    # get the geo data - city, country, country code using geocoder\n",
    "    gcode = reverse_geocoder.search( (lat, lon) )   \n",
    "    print(\"coords {}, {} are in country code {}\".format(lat, lon, gcode[0]['cc']))  \n",
    "    \n",
    "    # now let's format and set the name and country code values in \"place\" in the original tweets df,\n",
    "    # while making sure we edit the original df and not a copy\n",
    "    new_place_name = gcode[0]['admin1']\n",
    "    if not new_place_name:\n",
    "        new_place_name = gcode[0]['admin2']\n",
    "    if not new_place_name:\n",
    "        new_place_name = gcode[0]['name']\n",
    "\n",
    "    new_place = {\\\n",
    "        'name: ' + new_place_name, \\\n",
    "        'country_code: ' + gcode[0]['cc'] }\n",
    "    i = row['id']\n",
    "\n",
    "    tweets.loc[ tweets['id']==i ,'place'] = str(new_place)\n",
    "    #print(tweets.loc[ tweets['id']==i, 'place'])\n",
    "    \n",
    "# how long did that take?\n",
    "toc = time.perf_counter()\n",
    "print(f\"iterating and determining place from geo coords took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "gc.collect(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to make life easier, and to do this only once,\n",
    "# let's make some separate columns for later manipulation\n",
    "\n",
    "tic = time.perf_counter() # start a timer\n",
    "\n",
    "# add a column each for the hour number and the day number\n",
    "tweets['hour'] = tweets['created_at'].dt.hour\n",
    "tweets['day']  = tweets['created_at'].dt.day\n",
    "\n",
    "# let's also allocate space for country code and continent code columns here all at once\n",
    "# to hopefully reduce the amount of internal memory rearrangment as we fill in the values later\n",
    "tweets['cc']   = \"--\"\n",
    "tweets['cont_code'] = \"--\"\n",
    "\n",
    "# how long did that take?\n",
    "toc = time.perf_counter()\n",
    "print(f\"adding the day/hour and allocating ccode columns took {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.perf_counter() # start timer again\n",
    "\n",
    "# for other data, we need to (unfrotunately) iterate in order to read the dicts inside a column\n",
    "# and call multiple functions on them\n",
    "# so let's iterate once and do multiple things at a time while in that iterating loop   \n",
    "rows_updated = 0\n",
    "for i, row in tweets.iterrows():\n",
    "    try:\n",
    "        # get the country code column\n",
    "        ccode = eval(row['place'])['country_code']\n",
    "        #print(\"cc:\" + ccode)\n",
    "\n",
    "        # get the continent\n",
    "        continent_code = pcountry.country_alpha2_to_continent_code(ccode)\n",
    "        #print(\"continent:\" + continent_code)\n",
    "\n",
    "        # determine the local datetime from the country code so we can convert to local time\n",
    "        tz = pytz.country_timezones[ccode]\n",
    "        local_dt = row['created_at'].astimezone(tz[0])\n",
    "\n",
    "        # add the country code and continent code columns\n",
    "        tweets.at[i,'cc'] = ccode.upper()\n",
    "        tweets.at[i,'cont_code'] = continent_code.upper()\n",
    "\n",
    "        # add local hour and day columns\n",
    "        tweets.at[i,'local_hour'] = local_dt.hour\n",
    "        tweets.at[i,'local_day']  = local_dt.day\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    rows_updated += 1\n",
    "\n",
    "    # uncomment when testing code changes\n",
    "    #if rows_updated > 100:\n",
    "    #    break\n",
    "\n",
    "    # show progress every 1000 rows...\n",
    "    if rows_updated % 1000 == 0:\n",
    "        print(\"processed {} rows...\".format(rows_updated))\n",
    "       \n",
    "print(tweets.info())\n",
    "print(\"total rows updated: {}\".format(rows_updated))\n",
    "\n",
    "# now how long did the iteration take?\n",
    "toc = time.perf_counter()\n",
    "print(f\"iterating and determining country/continent took {toc - tic:0.4f} seconds, or {((toc - tic)/rows_updated):0.4f} per row\")\n",
    "\n",
    "gc.collect(2)"
   ]
  },
  {
   "source": [
    "# Save (or Load) Tweets DataFrame \n",
    "Since we have done so much time-consuming work so far, let's save or load from here\n",
    "in case the env crashes or you need to close and reopen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set this file name here when saving to pickle file\n",
    "latest_pickle_file = 'tweets_dataframe.202105111550.pickle'\n",
    "\n",
    "# uncomment this line to SAVE the dataframe to pickle file\n",
    "#tweets.to_pickle(latest_pickle_file)\n",
    "\n",
    "# uncomment this line to LOAD the dataframe from pickle file\n",
    "#tweets = pd.read_pickle(latest_pickle_file)"
   ]
  },
  {
   "source": [
    "# Looking at the data\n",
    "Now let's explore the data a little bit to get a feel for it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PICK A COUNTRY HERE \n",
    "# with two-letter country code \n",
    "# (for codes, see https://www.iban.com/country-codes)\n",
    "\n",
    "country_to_plot = 'AR'\n",
    "country_name = pcountry.country_alpha2_to_country_name(country_to_plot.upper())\n",
    "\n",
    "# select data based on that country code\n",
    "tweets_local = tweets[tweets['cc']==country_to_plot.upper()]\n",
    "\n",
    "# now plot that country's data by number of tweets per local hour of day\n",
    "if (not len(tweets_local) > 0):\n",
    "    print(\"no data to plot\")\n",
    "else:\n",
    "    try:\n",
    "        # use the groupby command to count tweets per hour, no matter the day\n",
    "        t1 = tweets_local.groupby([tweets_local.local_hour])['id'].count()\n",
    "        print(t1.describe())\n",
    "        #print(t)\n",
    "\n",
    "    except:\n",
    "        print(\"An error occured\")\n",
    "\n",
    "title_text = country_name + \" (\" + country_to_plot + \") \" + \"tweets per local hour of day, histogram\"\n",
    "ax = t1.plot.bar(title=title_text)\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"tweets_by_localhour_for_\" + country_to_plot + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at tweets per country\n",
    "\n",
    "# group by country code, with the values being the count of tweets per country\n",
    "t2 = tweets.groupby([tweets.cc])['id'].count()\n",
    "\n",
    "# drop any empty rows\n",
    "for i, j in t2.iteritems():\n",
    "    if (i == \"\") or (i == \"--\"):\n",
    "        t2.drop(i, inplace=True)\n",
    "\n",
    "print(t2.describe())\n",
    "\n",
    "# and plot the whole thing in alphabetical order\n",
    "# plot needs to be pretty big in order to get all the countries in, legibly\n",
    "ax2 = t2.plot.bar(figsize=(30,5), sort_columns=True, rot=90, title=\"Tweets by country\")\n",
    "ax2.set_xlabel('Country Code',fontdict={'fontsize':12})\n",
    "plt.rc('xtick', labelsize=8)    # fontsize of the tick labels\n",
    "plt.show()\n",
    "\n",
    "fig = ax2.get_figure()\n",
    "fig.savefig(\"tweets_by_country.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some countries are real outliers, making the majority tough to see\n",
    "# so let's switch to log scale on y axis to see the smaller ones\n",
    "ax2log = t2.plot.bar(figsize=(30,5), logy=True, sort_columns=True, rot=90, title=\"Tweets by country (log)\")\n",
    "ax2log.set_xlabel('Country Code',fontdict={'fontsize':12})\n",
    "\n",
    "# we'll need y axis major and minor grid lines to not lose the sense of enormous scale for th big countries\n",
    "ax2log.grid(axis='y')\n",
    "ax2log.grid('on', which='minor', axis='y' )\n",
    "\n",
    "plt.rc('xtick', labelsize=8)    # fontsize of the tick labels\n",
    "plt.show()\n",
    "\n",
    "fig = ax2log.get_figure()\n",
    "fig.savefig(\"tweets_by_country_logscale.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same exercise by continent\n",
    "\n",
    "# group by continent code, with the values being the count of tweets per country\n",
    "t3 = tweets.groupby([tweets.cont_code])['id'].count()\n",
    "print(t3.describe())\n",
    "\n",
    "# and plot the whole thing in alphabetical order\n",
    "# plot needs to be pretty big in order to get all the countries in, legibly\n",
    "ax3 = t3.plot.bar(figsize=(7,5), sort_columns=True, rot=90, title=\"Tweets by continent\")\n",
    "ax3.set_xlabel('Continent Code',fontdict={'fontsize':12})\n",
    "plt.rc('xtick', labelsize=8)    # fontsize of the tick labels\n",
    "plt.show()\n",
    "fig = ax3.get_figure()\n",
    "fig.savefig(\"tweets_by_continent.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can do a basic map\n",
    "gdf = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "gdf = gdf.assign(cc='--')\n",
    "gdf = gdf.assign(cont_code='--')\n",
    "\n",
    "rows_updated = 0\n",
    "for i, row in gdf.iterrows():\n",
    "    try:\n",
    "        try:\n",
    "            # get the country code column\n",
    "            ccode = pcountry.country_alpha3_to_country_alpha2(row['iso_a3'])\n",
    "        except:\n",
    "            try:\n",
    "                ccode = pcountry.country_name_to_country_alpha2(row['name'])\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        # get the continent\n",
    "        continent_code = pcountry.country_alpha2_to_continent_code(ccode)\n",
    "\n",
    "        # add the country code and continent code columns\n",
    "        gdf.at[i,'cc'] = ccode.upper()\n",
    "        gdf.at[i,'cont_code'] = continent_code.upper()\n",
    "    except:\n",
    "        print(\"error \" + row['name'] + \" \" + row['iso_a3'])\n",
    "\n",
    "\n",
    "gdf = gdf.merge(t2, how='left', on='cc')\n",
    "gdf.rename(columns = {'id':'num_tweets'}, inplace = True)\n",
    "gdf['num_tweets'] = gdf['num_tweets'].fillna(0)\n",
    "\n",
    "print('Number of rows:', len(gdf))\n",
    "\n",
    "# for debugging, tou can write a temporary csv file\n",
    "#tempdf=pd.DataFrame(gdf)\n",
    "#tempdf.to_csv(\"temp.csv\" )\n",
    "\n",
    "print(gdf.head())\n",
    "\n",
    "# Create a map\n",
    "fig, ax4 = plt.subplots(dpi=600)\n",
    "gdf.plot(ax=ax4, column='num_tweets', cmap='RdYlGn_r', legend=True)\n",
    "fig.savefig(\"world_tweets_by_country.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do a slightly different map; using log function to better visualize \n",
    "# across all countries instead of letting the top few countries dominate the scale\n",
    "import math\n",
    " \n",
    "gdf = gdf.assign(log_num_tweets=0.0)\n",
    "\n",
    "for i, row in gdf.iterrows():\n",
    "    try:\n",
    "        if float( row['num_tweets'] ) > 0:\n",
    "            gdf.at[i,'log_num_tweets'] = math.log10(float( row['num_tweets']) )\n",
    "        else:\n",
    "            gdf.at[i,'log_num_tweets'] = 0\n",
    "    except:\n",
    "        print(\"error \" + row['name'] + \" \" + str(row['num_tweets']))\n",
    "\n",
    "print(gdf.head())\n",
    "\n",
    "# Create a map\n",
    "fig, ax5 = plt.subplots(dpi=1200)\n",
    "gdf.plot(ax=ax5, column='log_num_tweets', cmap='RdYlGn_r', legend=True)\n",
    "fig.savefig(\"world_log_tweets_by_country.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's it!"
   ]
  }
 ]
}