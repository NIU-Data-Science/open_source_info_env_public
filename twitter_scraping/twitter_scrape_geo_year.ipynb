{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "52c912607f57bc3173c4fbc830b27eb01f22dcf7bbaa0ab7a7516b050c2157e2"
   }
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Twitter Scraping Using TWINT\n",
    "\n",
    "Twitter is erratic about giving out developer/API access codes.  If you can't get one, we can use twint for scraping instead.  As a bonus, we can easily get historical data.  \n",
    "\n",
    "In this script, we'll se the geo coords we want first, for those tweets tagged with geo coords,\n",
    "then run the data pull in chunks by year, convert to a dataframe, and save to a csv."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook calls functions stored in a different notebook - you may need to install\n",
    "#!pip install import-ipynb\n",
    "\n",
    "# then, you may need open and run twitter_scrape_by_twint.ipynb in your IDE or on \n",
    "# the same running python kernel before running this file\n",
    "#\n",
    "# if you make changes to that file, you will need to restart your kernel and maybe your IDE for changes to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports \n",
    "import twint \n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# import the more generic scraper function written and stored in the twitter_scrape.ipynb file\n",
    "import import_ipynb\n",
    "\n",
    "import twitter_scrape_by_twint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop function - takes a city and year and sets off the most reliable method to date\n",
    "# of coazing twint into giving us as many geo-tagged results as possible for that year given\n",
    "# all the problems and bugs in twint at the time of running.  \n",
    "# \n",
    "# Known problems discovered and worked around: since/until inconsistent, limit inconsistent but performs best at \n",
    "# 500, prefers to work backwards from an \"until\" date, twitter cuts you off if you call it too much\n",
    "#\n",
    "# Problems not worked around: 500 limit effectively limits you to 500*365 max per year; overlap between city ranges;\n",
    "# Twitter sometimes gives you back different results from run to run\n",
    "def twitter_scrape_by_geo_year(city_name = \"niamey\", which_year = 2021, use_tor_channel = False):\n",
    "    \"\"\"Given a city name and year, scrapes as many tweets as possible with twint and stores them in a csv\"\"\"\n",
    "    \n",
    "    geo_str = \"\"\n",
    "    city_name = city_name.lower()\n",
    "\n",
    "    # take the year and reorganize into \n",
    "    target_date_min, target_date_max = get_target_dates_min_and_max(which_year)\n",
    "\n",
    "    # here are the geo coordinates and ranges for a number of major cities in Niger\n",
    "    # the cities were chosen by size and interest\n",
    "    # Diffa and the Lake Chad Basin were left out of this analysis\n",
    "    if city_name == \"niamey\":\n",
    "         geo_str = \"13.5234,2.1167,75km\"\n",
    "    elif city_name == \"agadez\":\n",
    "        geo_str = \"16.9701,7.9856,75km\"\n",
    "    elif city_name == \"tillaberi\":\n",
    "        geo_str = \"14.2589,1.4671,75km\"\n",
    "    elif city_name == \"tahoua\":\n",
    "        geo_str = \"14.8939,5.2639,75km\"\n",
    "    elif city_name == \"dosso\":\n",
    "        geo_str = \"13.179,3.2071,75km\"\n",
    "    elif city_name == \"zinder\":\n",
    "        geo_str = \"13.804,8.9886,75km\"\n",
    "    elif city_name == \"maradi\":\n",
    "        geo_str = \"13.496,7.1081,75km\"\n",
    "    else:\n",
    "        raise Exception(\"city \\'{}\\' is not recognized\".format(city_name))\n",
    "\n",
    "    # instantiate twint\n",
    "    c = twint.Config()\n",
    "\n",
    "    c.Limit = 500           \n",
    "    c.Pandas = True\n",
    "    c.Debug = True\n",
    "    c.Count = True\n",
    "    c.Stats = True\n",
    "    c.Hide_output=True\n",
    "\n",
    "    c.Geo = geo_str\n",
    "    c.Until = target_date_max.isoformat()\n",
    "\n",
    "    print(\"will search {} tweet chunks back from 00:00am {}\".format(c.Limit, c.Until))\n",
    "\n",
    "    # if we want to use tor because we're getting locked out\n",
    "    if use_tor_channel == True:\n",
    "        # **optionally** run through the Tor browser \n",
    "        # just start up the main Tor browser and uncommen the below lines\n",
    "        print(\"using Tor\")\n",
    "        c.Proxy_host = \"127.0.0.1\"\n",
    "        c.Proxy_port = 9150\n",
    "        c.Proxy_type = \"socks5\"\n",
    "\n",
    "    # let's create a file name in a subdirectory\n",
    "    target_file_name = \"./\" + city_name.lower() + \"_geosearch\"\n",
    "\n",
    "    if not os.path.exists(target_file_name):\n",
    "        os.makedirs(target_file_name)\n",
    "\n",
    "    target_file_name = target_file_name + \"/\" + str(which_year) + \"_geo.csv\"\n",
    "    \n",
    "    # here is where we call the below twint operation in the twitter_scrap file\n",
    "    twitter_scrape.twitter_scrape_given_twint_config(c, target_date_min, target_date_max, target_file_name, city_name)\n"
   ]
  },
  {
   "source": [
    "# Main run block\n",
    "Run this block below to actually conduct the scraping and saving.  Generally this will be done one city name and one year at a time. but a loop is provided to run multiple years at a time.  Then manually change the year and/or city and run again.  One csv file per run.  Don't do this too fast or Twitter may refuse, throttle, or alter results.\n",
    "\n",
    "These runs can take a while.  Short pauses are built in to reduce chances of Twitter refusal.  Hopefully you only need to run this once, then run once a year after that to get a series of data for later analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_city = \"agadez\"      # options: \"niamey\" \"agadez\" \"tillaberi\" \"tahoua\" \"dosso\" \"zinder\" \"maradi\"\n",
    "use_tor = False \n",
    "\n",
    "# start at 2010 (there's nothing before that), going up to 2020 in this example\n",
    "for target_year in range(2010, 2020 + 1, 1):\n",
    "    \n",
    "    print(\"calling scrape for  year {}\".format(target_year))\n",
    "\n",
    "    twitter_scrape_by_geo_year(target_city, target_year, use_tor)\n",
    "    \n",
    "    print(\"done scraping year {}\".format(target_year))\n"
   ]
  }
 ]
}