{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Twitter Scraping Using TWINT\n",
    "\n",
    "Twitter Scraping Using TWINT\n",
    "Twitter is erratic about giving out developer/API access codes. If you can't get one, we can use twint for scraping instead. As a bonus, we can easily get historical data.\n",
    "\n",
    "In this script, we'll se the geo coords we want first, for those tweets tagged with geo coords, then run the data pull in chunks by year, convert to a dataframe, and save to a csv."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even with the scraping tools, Twitter keeps changing the url endpoint, and other  \n",
    "# access aspects, so we need to make sure to use the latest version of twint by \n",
    "# removing any previous versions and getting the latest direct from the github page.\n",
    "#\n",
    "# To do this, if you're having problems or errors, run this in conda:\n",
    "#\n",
    "# pip3 uninstall twint\n",
    "# pip3 install --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports \n",
    "import twint\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime \n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os \n",
    "# from collections import Counter\n",
    "\n",
    "# import and run nest_asyncio to prevent twint's asynchronous loop problems in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manage target dates and times\n",
    "def get_target_dates_min_and_max(t_year = 2020):\n",
    "    \"\"\"Function to manage target dates and times\"\"\"\n",
    "  \n",
    "    target_date_max = datetime.date(t_year+1,1,1)\n",
    "    target_date_min = datetime.date(t_year,  1,1)\n",
    "\n",
    "    print(\"target date range: {} 00:00am to {} 00:00am\".format(\n",
    "        target_date_min.isoformat(), \n",
    "        target_date_max.isoformat()))\n",
    "    \n",
    "    return target_date_min, target_date_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to conduct the scrape by calling Twint, assuming you already have a config to pass in\n",
    "\n",
    "# Since twint's 'Since' variable has inconsistent results, and Twitter returns results by most \n",
    "# recent first, and also Twitter will take a max date but not a max date/time,\n",
    "# we'll oddly chunk through some iterations of searches going back in history by\n",
    "# garbbing a big chunk, then restarting a couple seconds later at the most recent date not finished.\n",
    "# Later, at data analysis time, we'll need to clean it up by removing duplicates.\n",
    "\n",
    "def twitter_scrape_given_twint_config(c, target_date_min, target_date_max, output_filename = \"output.csv\", city_name = \"unknown\"):\n",
    "    \"\"\"Function to conduct the scrape by calling Twint, assuming you already have a config to pass in\"\"\"\n",
    "\n",
    "    least_recent_search_date = target_date_max  # this will keep us organized as we chunk through\n",
    "    most_recent_search_date = target_date_max\n",
    "\n",
    "    tweetcount = int(0)\n",
    "\n",
    "    # the big loop\n",
    "    while least_recent_search_date >  target_date_min:\n",
    "\n",
    "        print(\"running twint op max date {} 00:00am\".format(c.Until))\n",
    "\n",
    "        twint.run.Search(c)                     # go twint go\n",
    "    \n",
    "        df = twint.storage.panda.Tweets_df      # we'll use pandas dataframe for a quick organization\n",
    "\n",
    "        if int( len(df.index)) < 1:\n",
    "            print(\"WARNING: no data returned, breaking out of twint search loops\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            df['date'] = pd.to_datetime(df['date']) # type cast the whole column to make data manip easier\n",
    "\n",
    "        least_recent_search_date = df['date'].min()\n",
    "        most_recent_search_date = df['date'].max()\n",
    "\n",
    "        tweetcount = tweetcount + int( len(df.index) )  # running total \n",
    "\n",
    "        df[\"accessed_cityname\"] = city_name\n",
    "\n",
    "        # write the dataframe to a csv file by chunk\n",
    "        if os.path.exists(output_filename):\n",
    "            print(\"appending to file {}\".format(output_filename))\n",
    "            df.to_csv(output_filename, mode='a', header=False)\n",
    "        else:\n",
    "            print(\"creating file {}\".format(output_filename))\n",
    "            df.to_csv(output_filename, header=True)\n",
    "\n",
    "        print(\"date span collected: {} to {}\".format( least_recent_search_date, most_recent_search_date ))\n",
    "        print(\"total tweet count: {}\".format(tweetcount))\n",
    "\n",
    "        # change the date chunk\n",
    "        if least_recent_search_date.date() >= most_recent_search_date.date():\n",
    "            print(\"WARNING: more than {} records on day {}\".format(c.Limit, least_recent_search_date.date()))\n",
    "            print(\"  skipping remaining records on this date\")\n",
    "            c.Until = least_recent_search_date.date().isoformat()\n",
    "        else:\n",
    "            c.Until = (least_recent_search_date + datetime.timedelta(days=1)).date().isoformat()\n",
    "\n",
    "        time.sleep(3) # let's pause three seconds to avoid overloading Twitter\n",
    "\n",
    "    print(\"done\")\n"
   ]
  }
 ]
}